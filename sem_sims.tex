\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

%SetFonts

%SetFonts


\title{Self-regulation ontology project: Sampling simulations}
\author{Russell Poldrack}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Introduction}
At the kickoff meeting for this project on Nov 23, 2015, there was discussion regarding the most optimal sampling schemes for the proposed study.  The goal of this document is to lay out the work done to assess the questions raised in this discussion.

The original sampling plan was to attempt to enroll a number of MTurk workers to complete the entire battery of tasks (currently estimated at about 70 tasks).  This has the benefit of providing a relatively large number of subjects with substantial coverage of the full battery across ~ 10 sessions, but it also raises practical concerns.  In particular, the rate of attrition to be expected from this procedure is currently unknown, and a  high rate of non-completion of the full battery would raise difficult modeling issues, because the data would be missing not at random.  Dave Mackinnon suggested that it might be more optimal to instead sample a larger number of indviduals on a smaller portion of the battery using random assignment of tests to subjects, so that missing data would be missing completely at random, which allows the use of a much wider and more convenient set of modeling tools.  At the meeting it was decided that simulations were in order to assess this question.

\section{Simulation methods}
Simulations were based on real data obtained from the UCLA Consortium for Neuropsychiatric Phenomics (CNP).  This dataset consists of data from 1254 subjects across a large number of measures.  We selected a subset of these measures for further examination, including both performance and questionnaire measures relevant to the proposed work.  Initial exploration led to identification of a set of 12 variables (listed in Table \ref{tab:measures}, which were relatively well clustered into four clusters (see Figure \ref{fig:cluster}).  The total sample size for these measures (including only subjects with complete data for all measures) was 1126.

\begin{table}[ht]
\caption{Description of variables used in CFA model.}
\begin{center}
\begin{tabular}{|l|l|}
\hline
Factor 1& \\
\hline
scorei & Eyesenk Impulsiveness Inventory, Impulsivity Subscale\\
dysfunc\_total & Dickman, Dysfunctional Impusivity Subscale\\
bis\_factor2\_bi & Barratt Impulsiveness Scale, Behavioral Impulsivity factor\\
\hline
Factor 2& \\
\hline
ant\_conflict\_rt\_effect & Attention Networks Task, RT conflict effect\\
ts\_interference & Task switching interference effect\\
ddt\_total\_k & Delay Discounting task, mean discount rate\\
\hline
Factor 3& \\
\hline
vmnm\_manip\_dprime & Verbal WM manipulation, d-prime\\
scap\_dprime & Spatial working memory capacity \\
smnm\_manip\_dprime & Spatial WM manipulation, d-prime\\
\hline
Factor 4& \\
\hline
scorev & Eyesenk Impulsiveness Inventory, Venturesomeness Subscale\\
dysfunc\_total & Dickman, Functional Impusivity Subscale\\
persistence & TCI Persistence subscale\\
\hline
\end{tabular}
\end{center}
\label{tab:measures}
\end{table}%

\begin{figure}[!h]
\caption{\textbf{Clustering of the 12 variables of interest.}}
\centering
\includegraphics[width=0.75\linewidth]{iclust.pdf}
\label{fig:cluster}
\end{figure}

Based on these clusters, a confirmatory factor analysis (CFA) model was created, with four latent factors, each of which was associated with three behavioral measures.  In addition, a second model was created that includes only two factors and combines across measures that should clearly be separated; this is meant to serve as a comparison model, in order to asses the ability to identify the correct model.  These models are presented in Figure \ref{fig:models}.

\begin{figure}[!h]
\caption{\textbf{Correct (left) and incorrect (right) CFA models.}}
\centering
\includegraphics[width=1.\linewidth]{models.pdf}
\label{fig:models}
\end{figure}

Simulated datasets were generated by sampling subjects with replacement from the original 1126.  The base sample size for the simulations was 400 subjects, which was used for simulations with complete cases (no missing data).  Datasets with missing data were generated by sampling larger datasets and then setting a particular proportion of variables to NA for each subject; the sample size was increased in order to keep the total number of complete cells constant across simulations.  

The two CFA models were fit to each simulated dataset using the lavaan package in R. Two approaches to missing data were explored.  In the first, the missing data were imputed using two methods: predictive mean imputation with the mice package in R as well, and imputation using Amelia II.  The models were then fit to each imputed dataset with no missing values.  In the second, the model was fit using full information maximum likelihood (FIML) with no imputation (which is appropriate given that the data are missing completely at random).  The primary measures of interest were root mean square approximation error (RMSEA) which is a measure of goodness of fit of the correct model, and the p-value for the model comparison between the correct and incorrect models, which was used to estimate statistical power to distinguish correct vs. incorrect models.  For each model, 1000 simulations were run for four levels of sampling (100\%, 50\%, 25\%, and 20\% of tasks present for each subject).  

\section{Results}

Results of the simulations are presented in Figure \ref{fig:results}.  An important finding was that model fitting failed regularly, with increasing rates of failure as the amount of missing data increased.  Model fitting was substantially more successful when using FIML with missing data compared to analyses using imputation.  All subsequent analyses are conditionalized upon successful model fitting.

Divergent results were observed between analyses using imputation and those using FIML with missing data.  Model goodness of fit increased as the degree of subsampling increased for FIML, whereas it decreased (and was much lower overall) for models using imputation.  One concern is that a substantial number of models had an RMSEA value of zero, which may suggest model degeneracy even for models that did not fail outright (Figure \ref{fig:results}D).  

For model comparison, results also differed between missing data approaches. For models using imputation, there was very high power to identify the correct model even at the lowest levels of sampling; for models using FIML without imputation, power decreased as the proportion of sampled measures fell below 0.5, and fell below the nominal 80\% level at a sampling proportion of 20\%.  

\begin{figure}[!h]
\caption{\textbf{Results from simulations}}
\centering
\includegraphics[width=1.\linewidth]{results.pdf}
\label{fig:results}
\end{figure}

\section{Summary}
The foregoing results suggest that we can reasonably move to sampling a very small proportion of tasks per subject, though this decision depends on a complex set of tradeoffs between different figures of merit across the simulations.  In particular, 
%\subsection{}



\end{document}  